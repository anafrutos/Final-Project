{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3c8437",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adec964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae7a2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pip install command below if you don't already have the library\n",
    "# !pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "\n",
    "# Run the below command if you don't already have Pandas\n",
    "# !pip install pandas\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57515895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting variables to be used in format string command below\n",
    "tweet_count = 500\n",
    "text_query = \"its the elephant\"\n",
    "since_date = \"2020-06-01\"\n",
    "until_date = \"2020-07-31\"\n",
    "\n",
    "# Using OS library to call CLI commands in Python\n",
    "os.system('snscrape --jsonl --max-results {} --since {} twitter-search \"{} until:{}\"> text-query-tweets.json'.format(tweet_count, since_date, text_query, until_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349022a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "tweets_df = pd.read_json('text-query-tweets.json', lines=True)\n",
    "\n",
    "# Displays first 5 entries from dataframe\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d33222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "tweets_df.to_csv('text-query-tweets.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f6fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6395f832",
   "metadata": {},
   "source": [
    "Article Notebook for Scraping Twitter Using snscrape's Python Wrapper\n",
    "\n",
    "Package Github: https://github.com/JustAnotherArchivist/snscrape \n",
    "This notebook will be using the development version of snscrape\n",
    "Article Read-Along: https://medium.com/better-programming/how-to-scrape-tweets-with-snscrape-90124ed006af\n",
    "Notebook Author: Martin Beck\n",
    "Information current as of November, 28th 2020\n",
    "This notebook contains materials for scraping tweets from Twitter using snscrape's Python Wrapper\n",
    "Dependencies:\n",
    "Your Python version must be 3.8 or higher. The development version of snscrape will not work with Python 3.7 or lower. You can download the latest Python version here.\n",
    "Development version of snscrape, uncomment the pip install line in the below cell to pip install in the notebook if you don't already have it.\n",
    "Pandas, the dataframes allows easy manipulation and indexing of data, this is more of a preference but is what I follow in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a53b1a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tweet' object has no attribute 'user'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-05a88ab90f66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mtweets_list2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Creating a dataframe from the tweets list above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tweet' object has no attribute 'user'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list2 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('Cat since:2020-01-01 until:2020-07-31').get_items()):\n",
    "    if i>100:\n",
    "        break\n",
    "    tweets_list2.append([tweet.date, tweet.content, tweet.user])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Text', 'User'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2b8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa3b807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5e46b85e2cd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Display first 5 entries from dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtweets_df2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets_df2' is not defined"
     ]
    }
   ],
   "source": [
    "# Display first 5 entries from dataframe\n",
    "tweets_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ced7fda",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-540530eb38d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweets_df2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Lang'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets_df2' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_df2['Lang'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc3ae4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ee2afc85f814>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# visualizing the comments' languages a) quick and dirty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtweets_df2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'barh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets_df2' is not defined"
     ]
    }
   ],
   "source": [
    "# visualizing the comments' languages a) quick and dirty\n",
    "tweets_df2.Lang.value_counts(normalize=True).head(10).sort_values().plot(kind='barh', figsize=(9,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae135c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_df2['Text'][550]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f159938",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b79d91169e90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Export dataframe into a CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtweets_df2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'6000_tweets.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets_df2' is not defined"
     ]
    }
   ],
   "source": [
    "# Export dataframe into a CSV\n",
    "tweets_df2.to_csv('6000_tweets.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd234b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14043ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1478e00d",
   "metadata": {},
   "source": [
    "https://medium.com/swlh/how-to-scrape-tweets-by-location-in-python-using-snscrape-8c870fa6ec25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37749227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffe73758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our search term, using syntax for Twitter's Advanced Search\n",
    "search = '\"ecological footprint\"'\n",
    "\n",
    "# the scraped tweets, this is a generator\n",
    "scraped_tweets = sntwitter.TwitterSearchScraper(search).get_items()\n",
    "\n",
    "# slicing the generator to keep only the first 100 tweets\n",
    "sliced_scraped_tweets = itertools.islice(scraped_tweets, 3000)\n",
    "\n",
    "# convert to a DataFrame and keep only relevant columns\n",
    "df = pd.DataFrame(sliced_scraped_tweets)[['date', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd3b1fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>outlinks</th>\n",
       "      <th>outlinksss</th>\n",
       "      <th>tcooutlinks</th>\n",
       "      <th>tcooutlinksss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/faithelietown/status/14466...</td>\n",
       "      <td>2021-10-09 00:13:56+00:00</td>\n",
       "      <td>My ecological footprint is 9.6 GHA.  I will do...</td>\n",
       "      <td>1446629968105119747</td>\n",
       "      <td>faithelietown</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/bbvaOpenMind/status/144655...</td>\n",
       "      <td>2021-10-08 19:00:08+00:00</td>\n",
       "      <td>Any food produced on planet earth has an ecolo...</td>\n",
       "      <td>1446550998454050825</td>\n",
       "      <td>bbvaOpenMind</td>\n",
       "      <td>[https://bbva.info/3oGQd53]</td>\n",
       "      <td>https://bbva.info/3oGQd53</td>\n",
       "      <td>[https://t.co/aPG3ig5oKZ]</td>\n",
       "      <td>https://t.co/aPG3ig5oKZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/KartikeyaSingh/status/1446...</td>\n",
       "      <td>2021-10-08 18:45:04+00:00</td>\n",
       "      <td>More than #climateaction I'm excited that corp...</td>\n",
       "      <td>1446547207868530688</td>\n",
       "      <td>KartikeyaSingh</td>\n",
       "      <td>[https://www.bloomberg.com/news/articles/2021-...</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2021-0...</td>\n",
       "      <td>[https://t.co/kWO8ux6BB7]</td>\n",
       "      <td>https://t.co/kWO8ux6BB7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://twitter.com/yogawithnat/status/1446546...</td>\n",
       "      <td>2021-10-08 18:42:00+00:00</td>\n",
       "      <td>@tagaq Not too mention the cost or other ecolo...</td>\n",
       "      <td>1446546436959703066</td>\n",
       "      <td>yogawithnat</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://twitter.com/SESolar/status/14465358790...</td>\n",
       "      <td>2021-10-08 18:00:03+00:00</td>\n",
       "      <td>We consume 73% more resources today than the E...</td>\n",
       "      <td>1446535879066009606</td>\n",
       "      <td>SESolar</td>\n",
       "      <td>[http://spr.ly/6012JHGwu]</td>\n",
       "      <td>http://spr.ly/6012JHGwu</td>\n",
       "      <td>[https://t.co/X5XXSqQAn1]</td>\n",
       "      <td>https://t.co/X5XXSqQAn1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>https://twitter.com/SvxkdqVYxkirMJV/status/136...</td>\n",
       "      <td>2021-03-11 02:18:31+00:00</td>\n",
       "      <td>I just calculated my Ecological Footprint! How...</td>\n",
       "      <td>1369835094295203848</td>\n",
       "      <td>SvxkdqVYxkirMJV</td>\n",
       "      <td>[https://footprintcalculator.org]</td>\n",
       "      <td>https://footprintcalculator.org</td>\n",
       "      <td>[https://t.co/HGmG1y4Hxr]</td>\n",
       "      <td>https://t.co/HGmG1y4Hxr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>https://twitter.com/adeptislept/status/1369834...</td>\n",
       "      <td>2021-03-11 02:17:56+00:00</td>\n",
       "      <td>@bunnysaeran jfjdjd honestly my brain too ðŸ˜­ i ...</td>\n",
       "      <td>1369834949638053897</td>\n",
       "      <td>adeptislept</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>https://twitter.com/JamesRon1980/status/136976...</td>\n",
       "      <td>2021-03-10 21:28:31+00:00</td>\n",
       "      <td>@verelst_t @groenlinks @jesseklaver @WBHoekstr...</td>\n",
       "      <td>1369762112399106048</td>\n",
       "      <td>JamesRon1980</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>https://twitter.com/menlobear/status/136975818...</td>\n",
       "      <td>2021-03-10 21:12:55+00:00</td>\n",
       "      <td>AMAZING UPCOMING GUEST #4 \\n\\nIn an hour, I'm ...</td>\n",
       "      <td>1369758187692253186</td>\n",
       "      <td>menlobear</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>https://twitter.com/StreelyDan/status/13697512...</td>\n",
       "      <td>2021-03-10 20:45:30+00:00</td>\n",
       "      <td>@SlowdrawOK @austin_walker If you're \"constant...</td>\n",
       "      <td>1369751288926273539</td>\n",
       "      <td>StreelyDan</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "0     https://twitter.com/faithelietown/status/14466...   \n",
       "1     https://twitter.com/bbvaOpenMind/status/144655...   \n",
       "2     https://twitter.com/KartikeyaSingh/status/1446...   \n",
       "3     https://twitter.com/yogawithnat/status/1446546...   \n",
       "4     https://twitter.com/SESolar/status/14465358790...   \n",
       "...                                                 ...   \n",
       "2995  https://twitter.com/SvxkdqVYxkirMJV/status/136...   \n",
       "2996  https://twitter.com/adeptislept/status/1369834...   \n",
       "2997  https://twitter.com/JamesRon1980/status/136976...   \n",
       "2998  https://twitter.com/menlobear/status/136975818...   \n",
       "2999  https://twitter.com/StreelyDan/status/13697512...   \n",
       "\n",
       "                          date  \\\n",
       "0    2021-10-09 00:13:56+00:00   \n",
       "1    2021-10-08 19:00:08+00:00   \n",
       "2    2021-10-08 18:45:04+00:00   \n",
       "3    2021-10-08 18:42:00+00:00   \n",
       "4    2021-10-08 18:00:03+00:00   \n",
       "...                        ...   \n",
       "2995 2021-03-11 02:18:31+00:00   \n",
       "2996 2021-03-11 02:17:56+00:00   \n",
       "2997 2021-03-10 21:28:31+00:00   \n",
       "2998 2021-03-10 21:12:55+00:00   \n",
       "2999 2021-03-10 20:45:30+00:00   \n",
       "\n",
       "                                                content                   id  \\\n",
       "0     My ecological footprint is 9.6 GHA.  I will do...  1446629968105119747   \n",
       "1     Any food produced on planet earth has an ecolo...  1446550998454050825   \n",
       "2     More than #climateaction I'm excited that corp...  1446547207868530688   \n",
       "3     @tagaq Not too mention the cost or other ecolo...  1446546436959703066   \n",
       "4     We consume 73% more resources today than the E...  1446535879066009606   \n",
       "...                                                 ...                  ...   \n",
       "2995  I just calculated my Ecological Footprint! How...  1369835094295203848   \n",
       "2996  @bunnysaeran jfjdjd honestly my brain too ðŸ˜­ i ...  1369834949638053897   \n",
       "2997  @verelst_t @groenlinks @jesseklaver @WBHoekstr...  1369762112399106048   \n",
       "2998  AMAZING UPCOMING GUEST #4 \\n\\nIn an hour, I'm ...  1369758187692253186   \n",
       "2999  @SlowdrawOK @austin_walker If you're \"constant...  1369751288926273539   \n",
       "\n",
       "             username                                           outlinks  \\\n",
       "0       faithelietown                                                 []   \n",
       "1        bbvaOpenMind                        [https://bbva.info/3oGQd53]   \n",
       "2      KartikeyaSingh  [https://www.bloomberg.com/news/articles/2021-...   \n",
       "3         yogawithnat                                                 []   \n",
       "4             SESolar                          [http://spr.ly/6012JHGwu]   \n",
       "...               ...                                                ...   \n",
       "2995  SvxkdqVYxkirMJV                  [https://footprintcalculator.org]   \n",
       "2996      adeptislept                                                 []   \n",
       "2997     JamesRon1980                                                 []   \n",
       "2998        menlobear                                                 []   \n",
       "2999       StreelyDan                                                 []   \n",
       "\n",
       "                                             outlinksss  \\\n",
       "0                                                         \n",
       "1                             https://bbva.info/3oGQd53   \n",
       "2     https://www.bloomberg.com/news/articles/2021-0...   \n",
       "3                                                         \n",
       "4                               http://spr.ly/6012JHGwu   \n",
       "...                                                 ...   \n",
       "2995                    https://footprintcalculator.org   \n",
       "2996                                                      \n",
       "2997                                                      \n",
       "2998                                                      \n",
       "2999                                                      \n",
       "\n",
       "                    tcooutlinks            tcooutlinksss  \n",
       "0                            []                           \n",
       "1     [https://t.co/aPG3ig5oKZ]  https://t.co/aPG3ig5oKZ  \n",
       "2     [https://t.co/kWO8ux6BB7]  https://t.co/kWO8ux6BB7  \n",
       "3                            []                           \n",
       "4     [https://t.co/X5XXSqQAn1]  https://t.co/X5XXSqQAn1  \n",
       "...                         ...                      ...  \n",
       "2995  [https://t.co/HGmG1y4Hxr]  https://t.co/HGmG1y4Hxr  \n",
       "2996                         []                           \n",
       "2997                         []                           \n",
       "2998                         []                           \n",
       "2999                         []                           \n",
       "\n",
       "[3000 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(\n",
    "    '\"ecological footprint\"').get_items(), 3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(\n",
    "    'pizza near:\"Los Angeles\" within:10km').get_items(), 50))[['date', 'content']]\n",
    "df_city.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b63bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = '34.052235, -118.243683, 10km'\n",
    "df_coord = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(\n",
    "    'pizza geocode:\"{}\"'.format(loc)).get_items(), 50))[['date', 'content']]\n",
    "df_coord.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f79307",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = '34.052235, -118.243683, 10km'\n",
    "df_coord = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(\n",
    "    'pizza geocode:\"{}\"'.format(loc)).get_items(), 50))[['user', 'date','content']]\n",
    "\n",
    "df_coord['user_location'] =  df_coord['user'].apply(lambda x: x['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416280b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coord['user_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7265449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Text', 'Username', 'Lang', 'LikesCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop sentiment analysis classifier using traditional ML models\n",
    "# Pipeline modeling using the following guide: \n",
    "# https://ryan-cranfill.github.io/sentiment-pipeline-sklearn-1/\n",
    "# Data processing and cleaning guide:\n",
    "# https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, auc, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Dataset of 1.6m Twitter tweets\n",
    "columns = ['sentiment', 'id', 'date', 'query_string', 'user', 'text']\n",
    "train = pd.read_csv('stanford_twitter_train.csv', encoding='latin-1', header=None, names=columns)\n",
    "test = pd.read_csv('stanford_twitter_test.csv', encoding='latin-1', header=None, names=columns)\n",
    "## Local helpers\n",
    "\n",
    "# AUC visualization\n",
    "def show_roc(model, test, test_labels):\n",
    "    # Predict\n",
    "    probs = model.predict_proba(test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = roc_curve(test_labels, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Chart\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "# Tweet cleanser\n",
    "tok = nltk.tokenize.WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "def clean_tweet(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n",
    "# Data cleaning\n",
    "cleaned_tweets = []\n",
    "for tweet in train['text']:                                                                 \n",
    "    cleaned_tweets.append(clean_tweet(tweet))\n",
    "cleaned_df = pd.DataFrame(cleaned_tweets, columns=['text'])\n",
    "cleaned_df['target'] = train.sentiment\n",
    "cleaned_df.target[cleaned_df.target == 4] = 1 # rename 4 to 1 as positive label\n",
    "cleaned_df = cleaned_df[cleaned_df.target != 2] # remove neutral labels\n",
    "cleaned_df = cleaned_df.dropna() # drop null records\n",
    "cleaned_df.to_csv('stanford_clean_twitter_train.csv',encoding='utf-8')\n",
    "# Starting point from import\n",
    "csv = 'stanford_clean_twitter_train.csv'\n",
    "df = pd.read_csv(csv,index_col=0)\n",
    "# Random shuffle and ensure no null records\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df = df.dropna() # drop null records\n",
    "X, y = df.text[0:200000], df.target[0:200000] # Max data size 200k for memory purposes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.10)\n",
    "# Dataset shapes post-split\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))\n",
    "print(np.unique(y_train))\n",
    "(180000,)\n",
    "(20000,)\n",
    "[0 1]\n",
    "# NLTK Twitter tokenizer best used for short comment-type text sets\n",
    "import nltk\n",
    "tokenizer = nltk.casual.TweetTokenizer(preserve_case=False)\n",
    "# Hyperparameter tuning (Simple model)\n",
    "#cvect = CountVectorizer(tokenizer=tokenizer.tokenize)\n",
    "tfidf = TfidfVectorizer()\n",
    "clf = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('tfidf', tfidf),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__ngram_range': [(1,1), (1,2), (1,3)], # ngram range of tokenizer\n",
    "    'tfidf__norm': ['l1', 'l2', None], # term vector normalization\n",
    "    'tfidf__max_df': [0.25, 0.5, 1.0], # maximum document frequency for the CountVectorizer\n",
    "    'clf__C': np.logspace(-2, 0, 3) # C value for the LogisticRegression\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, cv=3, verbose=1)\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "t0 = time.time()\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time.time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "Performing grid search...\n",
    "pipeline: ['tfidf', 'clf']\n",
    "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
    "\n",
    "\n",
    "[Parallel(n_jobs=1)]: Done 243 out of 243 | elapsed: 52.7min finished\n",
    "\n",
    "\n",
    "done in 3186.295s\n",
    "\n",
    "Best score: 0.803\n",
    "Best parameters set:\n",
    "\tclf__C: 0.01\n",
    "\ttfidf__max_df: 0.25\n",
    "\ttfidf__ngram_range: (1, 3)\n",
    "\ttfidf__norm: None\n",
    "# Dump model from grid search cv\n",
    "joblib.dump(grid.best_estimator_, 'lr_sentiment_cv.pkl', compress=1)\n",
    "['lr_sentiment_cv.pkl']\n",
    "# Starting point 2: Post-model load comparison\n",
    "lra = joblib.load('./Models/Stanford_Twitter_Models/lr_sentiment_cv.pkl') \n",
    "lrb = joblib.load('./Models/Twitter_Simple_Models/lr_sentiment_basic.pkl') \n",
    "# Model performance indicators for basic model\n",
    "y_pred_basic = lrb.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_basic))\n",
    "show_roc(lrb, X_test, y_test) # AUC\n",
    "[[7562 2347]\n",
    " [2181 7910]]\n",
    "basic_auc\n",
    "# Model performance indicators for hypertuned model\n",
    "y_pred_hyper = lra.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_hyper))\n",
    "show_roc(lra, X_test, y_test) # AUC\n",
    "[[7861 2048]\n",
    " [1863 8228]]\n",
    "cv_auc\n",
    "print(lrb.predict([\"terrible idea why was this even made\"]))\n",
    "print(lrb.predict([\"that was the best movie ever\"]))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
